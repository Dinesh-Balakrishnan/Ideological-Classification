# Ideological-Classification: Analyzation

## Artifical Neural Networks /w Bag of Words

__Accuracy:__ 64.3%

The ANN model, when implementing the bag of words encoding scheme, uses the encoded sparse matrix as its input. As a result, a linear model will depend on the weight value of each word, with no existing correlation. Despite this, the model was able to reach a 60% accuracy using the linear model. The model was then improved by including a few dense layers equal to the length of the input. The model then reached 63% accuracy, most likely due to it being able to identify correlations between different words and determine words that have greater political implication. Lastly, two additional hidden layers were added to reduce the feature count to 1000 before generating the output; this increased the accuracy to 64.3%. Further reducing the feature count in the hidden layers reduced the accuracy. This could be because the data is highly intertwined and doesn't have distinct features. Instead, the classification is based on many minor differences between liberals and conservatives.

## Artificial Neural Networks /w Word 2 Vector

__Accuracy:__ 65.3%

Unlike the bag of words model, the current ANN model uses values generated by the Tokenizer class as input to an embedding layer that is then passed through a DNN. The best model was able to generate an accuracy of 65.3%, which is a slight improvement over the BoW ANN. The best dimensions chosen for the embedding layer were 16 dimensions, but this is most likely created due to the lack of data. If more data was available, it would be possible to increase the dimension count and extract more features from the training set.

## Model Classification /w Bag of Words

__Accuracy:__ 68%

Model classification simply involved using existing ML models to classify the data. There are three main cateogories I used during testing, neighbor-based models, decision-tree models, and support vector models. The neighbor-based models, such as Gaussian Naive Bayes and K-Neighbors, performed relatively poor. As stated before, the poor accuracy is a result of the data being highly intertwined; this means that liberal datapoints are often close to conservative datapoints. Decision-tree based classifiers also perform poorly due to this exact reason. It's almost impossible to categorize specific regions as liberal or conservative. The support vector machine did the best by choosing general regions to be classified as liberal or conservative. This is a better approach than decision trees, because the SVM isn't trying to find highly specific regions but rather categorizes the entire dataset.

## Model Classification /w Word 2 Vector

__Accuracy:__ 82%

When using the Word2Vec encoding scheme, the main difference to highlight is the change between the support vector machine. Due to the nature of the tokenized input, the order in which the datapoints appear now impacts the machine-learning models. SVM doesn't take ths into account and now has a worse accuracy of 65%. Alternatively, decision trees perform much better. Firstly, the input length is much smaller than when using BoW, which helps reduce decision tree overfitting. Random forest can then use the individual input values and their position within the tokenized input array to create better classifications. XGBoost completely outperforms the other models, because it generates decision trees for a single value within the input array, then improves its classification by generating decision tress for the next value within the input array. This ordered training process is ideal for the ordered structure created by the input from the Word2Vec model.
